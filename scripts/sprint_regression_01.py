import numpy as np
import pandas as pd

from sklearn import model_selection, preprocessing

from sklearn.model_selection import (
    cross_val_predict,
    cross_val_score,
    cross_validate,
    train_test_split,
)
from sklearn.linear_model import (
    LinearRegression,
    LassoCV,
    RidgeCV,
    ElasticNetCV,
)
from sklearn.metrics import mean_squared_error

import matplotlib.pyplot as plt
import seaborn as sns

from IPython.display import display

import warnings

warnings.filterwarnings("ignore")


# Configuration pour un affichage plus riche
pd.set_option("display.max_columns", None)
pd.set_option("display.max_rows", 50)
pd.set_option("display.width", None)
pd.set_option("display.max_colwidth", 50)


def display_dataframe_info(df, title="DataFrame Info"):
    """Affiche les informations du DataFrame de mani√®re format√©e"""
    print("=" * 80)
    print(f"üìä {title}")
    print("=" * 80)

    # Informations de base
    print(
        f"üìã Forme du DataFrame: {df.shape[0]} lignes √ó {df.shape[1]} colonnes"
    )
    print(
        f"üíæ Utilisation m√©moire: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB"
    )
    print()

    # Types de donn√©es
    print("üîç Types de donn√©es:")
    print("-" * 40)
    for col, dtype in df.dtypes.items():
        print(f"  {col:<25} | {dtype}")
    print()

    # Valeurs manquantes
    missing_values = df.isnull().sum()
    if missing_values.sum() > 0:
        print("‚ùå Valeurs manquantes:")
        print("-" * 40)
        for col, missing in missing_values.items():
            if missing > 0:
                percentage = (missing / len(df)) * 100
                print(f"  {col:<25} | {missing:>6} ({percentage:>5.1f}%)")
        print()
    else:
        print("‚úÖ Aucune valeur manquante d√©tect√©e")
        print()

    # Aper√ßu des donn√©es
    print("üëÄ Aper√ßu des donn√©es (5 premi√®res lignes):")
    print("-" * 40)
    print(df.head().to_string())
    print()

    # Statistiques descriptives pour les colonnes num√©riques
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        print("üìà Statistiques descriptives (colonnes num√©riques):")
        print("-" * 40)
        print(df[numeric_cols].describe().to_string())
        print()

    # Valeurs uniques pour les colonnes cat√©gorielles (limit√©es)
    categorical_cols = df.select_dtypes(include=["object"]).columns
    if len(categorical_cols) > 0:
        print("üè∑Ô∏è  Aper√ßu des valeurs uniques (colonnes cat√©gorielles):")
        print("-" * 40)
        for col in categorical_cols[
            :5
        ]:  # Limite √† 5 colonnes pour √©viter l'encombrement
            unique_count = df[col].nunique()
            print(f"  {col}: {unique_count} valeurs uniques")
            if unique_count <= 10:
                print(f"    Valeurs: {list(df[col].unique())}")
            else:
                print(f"    Exemples: {list(df[col].unique()[:5])}...")
        print()


# Fonction pour afficher des informations sp√©cifiques sur les colonnes
def explore_column(df, column_name):
    """Explore une colonne sp√©cifique du DataFrame"""
    if column_name not in df.columns:
        print(f"‚ùå La colonne '{column_name}' n'existe pas.")
        return

    print(f"\nüîç Exploration de la colonne: {column_name}")
    print("-" * 50)
    print(f"Type: {df[column_name].dtype}")
    print(f"Valeurs uniques: {df[column_name].nunique()}")
    print(f"Valeurs manquantes: {df[column_name].isnull().sum()}")

    if df[column_name].dtype == "object":
        print("\nValeurs les plus fr√©quentes:")
        print(df[column_name].value_counts().head(10))
    elif df[column_name].dtype in ["int64", "float64"]:
        print(f"\nMin: {df[column_name].min()}")
        print(f"Max: {df[column_name].max()}")
        print(f"Moyenne: {df[column_name].mean():.2f}")
        print(f"M√©diane: {df[column_name].median():.2f}")


# Charger le dataset
df = pd.read_csv("../data/CarPrice_Assignment.csv")

# Afficher les informations du DataFrame
display_dataframe_info(df)

# Visualiser la relation entre 'curb-weight' et 'price'
df.rename(columns={'curbweight': 'curb-weight'}, inplace=True)
# Afficher des informations sp√©cifiques sur certaines colonnes  
explore_column(df, 'curb-weight')
explore_column(df, 'price')

# Visualisation de la relation entre 'curb-weight' et 'price'
# plt.style.use('seaborn-darkgrid')
# fig = plt.figure(figsize=(5, 5))
# ax = fig.add_subplot(111)
# ax.scatter(x=df['curb-weight'], y=df['price'])
sns.relplot(x=df['curb-weight'], y=df['price'])
plt.show()

# S√©lectionner les colonnes pour X (data) et y (target)
X = df[['curb-weight']] # renvoie un DataFrame pandas, equivalent √† X = pd.DataFrame(df['curb-weight'])
y = df['price'] # renvoie une s√©rie pandas

# Ins√©rez votre code ici 
slr = LinearRegression()

# # S√©paration des donn√©es d'entrainement et de test
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=33)

# # standardisation des donn√©es
# scaler = preprocessing.StandardScaler()
# X_train[X_train.columns] = pd.DataFrame(scaler.fit_transform(X_train), index=X_train.index)
# X_test[X_test.columns] = pd.DataFrame(scaler.transform(X_test), index=X_test.index)

# Entrainement
slr.fit(X, y)

# Metriques simples
# print("score R¬≤ (coefficient de d√©termination) de l'environnement train :",
#       slr.score(X_train, y_train))
# print("score R¬≤ (coefficient de d√©termination) de l'environnement test  :",
#       slr.score(X_test, y_test))

# pr√©diction
# y_pred_train = slr.predict(X_train)
# y_pred_test = slr.predict(X_test)
# print('rmse train :', np.sqrt(mean_squared_error(y_train, y_pred_train)))
# print('rmse test : ', np.sqrt(mean_squared_error(y_test, y_pred_test)))

# Affichage des coefficients
print("Coefficients de la r√©gression lin√©aire simple :")
print("ordonn√©e √† l'origine : ", slr.intercept_)
print("pente ou coefficient de la droite : ", slr.coef_)

# Validation crois√©e √† 4 plis ou echantillons
scores = cross_validate(slr, X, y, return_train_score=True, cv=4)

for key in scores.keys():
    print(f"{key}: {scores[key]}")

# Moyennes
print("Moyenne test :", scores['test_score'].mean())
print("Moyenne entra√Ænement :", scores['train_score'].mean())

# Pr√©diction variables ajust√©es et r√©sidus
y_pred_prix = slr.predict(X)
residus = y_pred_prix - y
display(residus.describe())

# Graphique de r√©gression
plt.figure(figsize=(10, 8))
plt.scatter(X, y, color='darkblue', label='Donn√©es r√©elles')
plt.plot(X, y_pred_prix, color='k', label='R√©gression lin√©aire simple X / y Pr√©dit')
plt.legend()
plt.xlabel('Curb Weight')
plt.ylabel('Price')
plt.title('R√©gression Lin√©aire Simple : Curb Weight vs Price')
plt.show()

# Graphique des r√©sidus
plt.scatter(y, residus, color='#980a10', s=15)
plt.plot((y.min(), y.max()), (0, 0), lw=3, color='#0a5798')
plt.show()

# V√©rification de la normalit√© des r√©sidus
import scipy.stats as stats
# normalisation des r√©sidus
residus_norm = (residus-residus.mean())/residus.std()
# qq plot
stats.probplot(residus_norm, plot=plt)
plt.show()

# Histogramme des r√©sidus
plt.hist(residus, bins=30)
plt.title('Histogramme des r√©sidus')
plt.show()


# Importer la fonction f_regression pour la s√©lection de caract√©ristiques
from sklearn.feature_selection import f_regression

# Appliquer f_regression
f_scores, p_values = f_regression(X, y)

# Affichage
df_resultats = pd.DataFrame({
    'Feature': [f'X{i}' for i in range(X.shape[1])],
    'F-score': f_scores,
    'p-value': p_values
})

display(df_resultats)

# RMSE (Root Mean Squared Error)
def rmse(predictions, targets):
    return np.sqrt(((predictions - targets)**2).mean())


# Calculer le RMSE pour les pr√©dictions
print(rmse(y_pred_prix, y))

# Validation crois√©e avec pr√©dictions
y_pred_prix_2 = cross_val_predict(slr, X, y, cv=4)

print(rmse(y_pred_prix_2, y))